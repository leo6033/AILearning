<center><font size=8>论机器学习中的那些树——决策树</font></center>

从零基础开始参加了几场数据挖掘方面的比赛，每次比赛都会学到不少东西，自从上次在 `elo` 的 kernel 中看见很多人都使用 `LightGBM、XGBoost`，那之后我也开始用起了这些，但是却从未花时间去了解过这是究竟是什么，其内部工作原理是怎么样的，正好这段时间在参加`df平台`的`消费者人群画像—信用智能评分`这一比赛，做起了调参，但因为对其内部并不是很收悉，便准备好好学习有关树模型方面的内容，并写下这系列的博客。这里将从最基础的决策树开始讲起。

# 概述

决策树（decision tree）是一类常见的机器学习方法。类似于流程图，一颗决策树包含一个根节点、若干个内部节点和叶子节点，每一个树节点表示对一个属性的测试，每一个分支代表一个属性的输出，每一个叶子节点对应一种决策结果。从根节点到每个叶节点的路径对应了一个判定测试序列。其学习的基本流程遵循分治（divide-and-conquer）策略。

# 算法

**输入：**训练集$D=\{(x_1,y_1),(x_2,y_2),... ,(x_n,y_n)\}​$
	    属性集$A=\{a_1,a_2,...,a_n\}\\​$
**过程：**函数$TreeGenerate(D,A)​$

$1：生成节点 node;​$

$2：if$ $ D $ $ 中样本全属于同一类别$ $  C$ $ then$

$3：\quad将$ $ node$ $标 记为$ $ C$ $ 类叶节点;$ $ return$

$4：end​$ $ if​$

$5：if$ $ A=\empty $ $ OR$ $ D$ $ 中样本在A上取值相同$ $ then $

$6：\quad 将node标记为叶节点，其类别标记为D中样本数最多的类；then​$

$7：end​$ $if​$

$8:从A中选择最优划分属性​$

$9：for$ $ a_*$ $的每一个值$ $ a_{*}^{v}$ $ do$

$10:\quad 为node生成一个分支；令D_v表示D中在a_*上取值为a_{*}^{v} 的样本子集：​$

$11:\quad if $ $D_v$ $为空$ $then$

$12：\quad\quad将分支节点标记为叶节点，其类别标记为D中样本最多的类；return​$

$13：\quad else$

$14：\quad\quad以TreeGenerate(D_v,A​$ \ $\{a_*\})为分支节点​$

$15：\quad end$ $if​$

$16：end​$ $for​$

**输出：**以$node​$为节点的一颗决策树

# 划分选择

从上面的算法中可以看出，最重要的一步就是第8行选取最优划分，但我们该如何选取最优划分呢？这里就涉及到了信息增益的概念。在讲解信息增益前，先来了解了解信息熵和条件熵。

信息熵（information entropy）是度量样本集合纯度最常用的一种指标，它可以衡量一个随机变量出现的期望值。如果信息的不确定性越大，熵的值也就越大，出现的各种情况也就越多。

假设当前样本集合$D​$中第$k​$类样本所占比例为$p_k(k=1,2,\cdots,|n|)​$，则$D​$的信息熵为：
$$
Ent(D=-\sum_{k=1}^{|n|}p_k\log_2p_k) \tag{1}
$$
条件熵（conditional entropy）表示在已知随机变量X的条件下随机变量Y的不确定性。定义X给定条件下Y的条件概率分布的熵：
$$
Ent(Y|X)=\sum_{k=1}^{n}p_kH(Y|X=x_k) \tag{2}
$$
假定离散属性a有V个可能取值$\{a^1,a^2,\cdots,a^V\}$，若使用a来对样本集D进行划分，则会产生V个信息增益（information gain）